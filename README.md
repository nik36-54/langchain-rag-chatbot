# ğŸ” LangChain RAG Chatbot â€“ Production-Ready AI Chat Assistant

## **Overview**

This project is a production-ready **LLM-powered chatbot** built using:

- ğŸ§  **LangChain** for Retrieval-Augmented Generation (RAG)
- ğŸ“„ **Vector DB (Chroma)** for document chunk search
- âš¡ **FastAPI** as the backend API
- ğŸ–¥ï¸ **Streamlit** frontend for real-time interaction
- ğŸš€ **Railway** for full-stack deployment

It allows users to query large PDF or text documents (up to 100MB+) and get accurate, contextual answers in milliseconds.

---

## ğŸ’¡ Key Features

| Feature                                     | Description                                                           |
| ------------------------------------------- | --------------------------------------------------------------------- |
| ğŸ§  **RAG Pipeline**                         | Combines LLM with vector search for grounded, context-aware responses |
| âš¡ **Fast Response**                        | Sub-second answers using GPT-4 Turbo and Chroma                       |
| ğŸ“„ **Large Document Support**               | Efficient chunking, embedding, and retrieval for PDFs/Markdowns       |
| ğŸ’¬ **Streamlit Interface**                  | Chat-like UI to interact with backend                                 |
| ğŸ” **Environment-based API Key Management** | Secure use of OpenAI API                                              |
| ğŸ“¦ **Production Deployable**                | Fully deployable via Railway (frontend + backend)                     |

---

## ğŸ§ª Benchmark Highlights

| Metric         | Value                              |
| -------------- | ---------------------------------- |
| Document Size  | 100MB PDF                          |
| Indexed Chunks | 12,000+                            |
| Query Latency  | ~500â€“800ms                         |
| Confidence     | 0.85+ (based on vector similarity) |
| Cold Start     | ~3s on free tier (Railway)         |

---

## ğŸ“ Tech Stack

- **LangChain**: RAG orchestration
- **OpenAI**: LLM + Embeddings
- **Chroma DB**: Vector similarity search
- **FastAPI**: Backend API
- **Streamlit**: Frontend interface
- **Railway**: Deployment platform

---

## ğŸš€ Use Cases

- Technical documentation Q&A assistant
- Internal knowledge base chatbot
- Legal/Policy document search bot
- Customer support agent with contextual memory

## Project Structure

```
.
â”œâ”€â”€ backend/           # FastAPI + LangChain + Supabase backend
â”‚   â”œâ”€â”€ app/          # Application code
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ Procfile
â”‚   â””â”€â”€ .env
â””â”€â”€ frontend/         # Streamlit frontend
    â”œâ”€â”€ app.py        # Main Streamlit application
    â”œâ”€â”€ requirements.txt
    â”œâ”€â”€ Procfile
    â””â”€â”€ .env
```

## Features

- ğŸ¤– LLM-powered conversational AI
- ğŸ”’ Secure authentication with Supabase
- ğŸš€ FastAPI backend for robust API endpoints
- ğŸ’» Modern Streamlit frontend
- ğŸ“š LangChain for advanced language model interactions

## Setup

### Backend Setup

1. Navigate to the backend directory:

   ```bash
   cd backend
   ```

2. Create a virtual environment:

   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. Install dependencies:

   ```bash
   pip install -r requirements.txt
   ```

4. Set up environment variables in `.env`

5. Run the backend:
   ```bash
   uvicorn app.main:app --reload
   ```

### Frontend Setup

1. Navigate to the frontend directory:

   ```bash
   cd frontend
   ```

2. Create a virtual environment:

   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. Install dependencies:

   ```bash
   pip install -r requirements.txt
   ```

4. Set up environment variables in `.env`

5. Run the frontend:
   ```bash
   streamlit run app.py
   ```

## Environment Variables

### Backend (.env)

```
OPENAI_API_KEY=your_openai_api_key
SUPABASE_URL=your_supabase_url
SUPABASE_KEY=your_supabase_key
```

### Frontend (.env)

```
BACKEND_URL=http://localhost:8000
```

## Contributing

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Create a Pull Request

## License

MIT License
